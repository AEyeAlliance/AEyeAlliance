{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Braille to text converter",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "19cLVbAvTfuu0pqaadxpgyMsFxHC3nnNF",
          "timestamp": 1527465087778
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "T20hE89hygNL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This CNN model was trained on our old dataset that included only 78 images. "
      ]
    },
    {
      "metadata": {
        "id": "H-ra1DTnR0S6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Install pytorch "
      ]
    },
    {
      "metadata": {
        "id": "ikmT9ejsWLrU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "cellView": "code",
        "outputId": "6a12f54a-57d1-4a58-df5d-46e87112abe8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528300164826,
          "user_tz": 240,
          "elapsed": 4281,
          "user": {
            "displayName": "Ting-Yi Su",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118326931190013634369"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# use cross entropy for classification problems\n",
        "\n",
        "import time\n",
        "import platform\n",
        "import io\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from matplotlib.pyplot import cm \n",
        "\n",
        "# import KFold from scikit-learn\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def install_pytorch():\n",
        "    os = platform.system()\n",
        "    if os == \"Linux\":\n",
        "        !pip3 install http://download.pytorch.org/whl/cu90/torch-0.4.0-cp36-cp36m-linux_x86_64.whl\n",
        "    elif os == \"Windows\":\n",
        "        !pip3 install http://download.pytorch.org/whl/cu90/torch-0.4.0-cp36-cp36m-win_amd64.whl \n",
        "    !pip3 install torchvision\n",
        "\n",
        "\n",
        "# Install PyTorch.\n",
        "install_pytorch()\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==0.4.0 from http://download.pytorch.org/whl/cu90/torch-0.4.0-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BK-KXPMNRbao",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Enable GPU Usage\n",
        "\n",
        "Models trains much more quickly with GPU. "
      ]
    },
    {
      "metadata": {
        "id": "wvckHX5NktK7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "932b551d-abe1-49b6-a9ce-9a8610014bd7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528299585839,
          "user_tz": 240,
          "elapsed": 306,
          "user": {
            "displayName": "Ting-Yi Su",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118326931190013634369"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# turn on GPU\n",
        "use_gpu = torch.cuda.is_available()\n",
        "use_gpu = False\n",
        "print(\"GPU Available: {}\".format(use_gpu))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU Available: False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TuhFUrX0O2Se",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ]
    },
    {
      "metadata": {
        "id": "Tw-iCQFfRhS7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import and read the CSV file "
      ]
    },
    {
      "metadata": {
        "id": "_1kJYrn79Cft",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#import csv dataset\n",
        "import torchvision\n",
        "import csv\n",
        "import os\n",
        "import pandas as pd \n",
        "from urllib import request\n",
        "import requests\n",
        "\n",
        "# Upload and read the csv file from the github repo\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/HelenG123/aeye-alliance/master/Data/data_day3.csv\")\n",
        "df_test = pd.read_csv(\"https://raw.githubusercontent.com/HelenG123/ai-alliance/master/brailleFinalv2.csv\")\n",
        "\n",
        "#https://raw.githubusercontent.com/HelenG123/ai-alliance/master/braille_data.csv\n",
        "# # Read the CSV file from a local directory\n",
        "# dataset_name = list(dataset.keys())[0]\n",
        "# df = pd.read_csv(io.StringIO(dataset[dataset_name].decode('utf-8')))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NGxuIdjcd4-p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generate the Targets\n",
        "Create a dictionary that contains the target number for each image in the Braille alphabet.\n"
      ]
    },
    {
      "metadata": {
        "id": "dbQyhtDHnB6o",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "f44a7b02-ddf2-4001-9819-6034557cde95",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528299588323,
          "user_tz": 240,
          "elapsed": 273,
          "user": {
            "displayName": "Ting-Yi Su",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118326931190013634369"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# not using one hot encoding vector\n",
        "# using 1-26 to represent a-z instead\n",
        "\n",
        "import string\n",
        "\n",
        "target = {}\n",
        "alphabet = list(string.ascii_lowercase)\n",
        "\n",
        "number = 0 \n",
        "for letter in alphabet: \n",
        "  target[letter] = number\n",
        "  number += 1\n",
        "\n",
        "print(target)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TLSF1BrXd5sU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# import string\n",
        "# alphabet = list(string.ascii_lowercase)\n",
        "\n",
        "# target = {}\n",
        "\n",
        "# # Initalize a target dict that has the letters as its keys and as its value\n",
        "# # an empty one-hot encoding of size 26\n",
        "# for letter in alphabet: \n",
        "#   target[letter] = [0] * 26\n",
        "\n",
        "# # Do the one-hot encoding for each letter now \n",
        "# curr_pos = 0 \n",
        "# for curr_letter in target.keys():\n",
        "#   target[curr_letter][curr_pos] = 1\n",
        "#   curr_pos += 1  \n",
        "\n",
        "# print(target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S6XGJhcJOvCM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Iterate over the CSV files to add the targets\n",
        "Create a dictionary of the images that contains the image as a Tensor and its target as a number between 1-26."
      ]
    },
    {
      "metadata": {
        "id": "k6W-EK9_Oz_C",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 932
        },
        "outputId": "fc8cfc6f-b218-4782-d4bc-19e05aa4c37f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528301089796,
          "user_tz": 240,
          "elapsed": 81821,
          "user": {
            "displayName": "Ting-Yi Su",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118326931190013634369"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from scipy import misc\n",
        "from io import BytesIO\n",
        "import urllib\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "data=[]\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "  picture = []\n",
        "  url = row['Labeled Data']\n",
        "  label = row['Label']\n",
        "  curr_target = target[label[10]]\n",
        "  # print(curr_target)\n",
        "\n",
        "  x = urllib.request.urlopen(url)\n",
        "  resp = x.read()\n",
        "  image = np.array(bytearray(resp), dtype=np.uint8)\n",
        "  image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "  # resize image to 28x28x3\n",
        "  image = cv2.resize(image, (28, 28))\n",
        "  image = image.astype(np.float32)/255.0\n",
        "   #image = image.flatten().astype(np.float32)/255.0\n",
        "  image = torch.from_numpy(image)\n",
        "  picture.append(image)\n",
        "  curr_target=torch.LongTensor([curr_target])\n",
        "  picture.append(curr_target)\n",
        "  data.append(picture)\n",
        "\n",
        "print(image.shape) # these are the dimensions of our image\n",
        "print(data[0][0])\n",
        "print(data[0][1])\n",
        "print(max([d[1] for d in data]))\n",
        "\n",
        "data_test=[]\n",
        "\n",
        "for i, row in df_test.iterrows():\n",
        "  picture = []\n",
        "  url = row['Labeled Data']\n",
        "  label = row['External ID']\n",
        "  curr_target = target[label[0]]\n",
        "  # print(curr_target)\n",
        "\n",
        "  x = urllib.request.urlopen(url)\n",
        "  resp = x.read()\n",
        "  image = np.array(bytearray(resp), dtype=np.uint8)\n",
        "  image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "  # resize image to 28x28x3\n",
        "  image = cv2.resize(image, (28, 28))\n",
        "  image = image.astype(np.float32)/255.0\n",
        "   #image = image.flatten().astype(np.float32)/255.0\n",
        "  image = torch.from_numpy(image)\n",
        "  picture.append(image)\n",
        "  curr_target=torch.LongTensor([curr_target])\n",
        "  picture.append(curr_target)\n",
        "  data_test.append(picture)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([28, 28, 3])\n",
            "tensor([[[ 0.8745,  0.8314,  0.8157],\n",
            "         [ 0.8745,  0.8275,  0.8157],\n",
            "         [ 0.8745,  0.8275,  0.8196],\n",
            "         ...,\n",
            "         [ 0.8784,  0.8314,  0.8157],\n",
            "         [ 0.8745,  0.8314,  0.8196],\n",
            "         [ 0.8745,  0.8314,  0.8196]],\n",
            "\n",
            "        [[ 0.8706,  0.8275,  0.8118],\n",
            "         [ 0.8706,  0.8275,  0.8118],\n",
            "         [ 0.8667,  0.8235,  0.8078],\n",
            "         ...,\n",
            "         [ 0.8824,  0.8353,  0.8196],\n",
            "         [ 0.8745,  0.8275,  0.8196],\n",
            "         [ 0.8706,  0.8235,  0.8157]],\n",
            "\n",
            "        [[ 0.8745,  0.8314,  0.8157],\n",
            "         [ 0.8745,  0.8314,  0.8157],\n",
            "         [ 0.8706,  0.8275,  0.8118],\n",
            "         ...,\n",
            "         [ 0.8863,  0.8392,  0.8275],\n",
            "         [ 0.8745,  0.8275,  0.8196],\n",
            "         [ 0.8745,  0.8275,  0.8196]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.8706,  0.8314,  0.8039],\n",
            "         [ 0.8824,  0.8431,  0.8157],\n",
            "         [ 0.8784,  0.8392,  0.8118],\n",
            "         ...,\n",
            "         [ 0.8706,  0.8314,  0.8039],\n",
            "         [ 0.8627,  0.8275,  0.8000],\n",
            "         [ 0.8627,  0.8275,  0.7961]],\n",
            "\n",
            "        [[ 0.8627,  0.8275,  0.7922],\n",
            "         [ 0.8745,  0.8392,  0.8039],\n",
            "         [ 0.8745,  0.8392,  0.8039],\n",
            "         ...,\n",
            "         [ 0.8784,  0.8392,  0.8118],\n",
            "         [ 0.8627,  0.8235,  0.7961],\n",
            "         [ 0.8588,  0.8196,  0.7922]],\n",
            "\n",
            "        [[ 0.8667,  0.8314,  0.7961],\n",
            "         [ 0.8745,  0.8392,  0.8039],\n",
            "         [ 0.8745,  0.8392,  0.8039],\n",
            "         ...,\n",
            "         [ 0.8667,  0.8275,  0.7961],\n",
            "         [ 0.8667,  0.8275,  0.8000],\n",
            "         [ 0.8667,  0.8275,  0.8000]]])\n",
            "tensor([ 0])\n",
            "tensor([ 25])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "thE4Nl9nMP0d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the dataset\n",
        "\n",
        "Dataloader gives the object that we can iterate over for enumerating and training our data."
      ]
    },
    {
      "metadata": {
        "id": "jAGps6lEMXVC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e20909e6-476b-4c75-bcd7-65f914cb9146",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528301254789,
          "user_tz": 240,
          "elapsed": 310,
          "user": {
            "displayName": "Ting-Yi Su",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118326931190013634369"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "batch_size_test = 5\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=data_test, batch_size=batch_size_test, shuffle=False)\n",
        "\n",
        "print(len(train_loader))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "69\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zUxdGGHGeajB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize the Image\n",
        "Demonstrate that we can access and display an image from the dataset. \n"
      ]
    },
    {
      "metadata": {
        "id": "UkvrVL6EaJ0Z",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "c1e75192-b603-47bc-e5c9-ff29a7471e7f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528300357090,
          "user_tz": 240,
          "elapsed": 443,
          "user": {
            "displayName": "Ting-Yi Su",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118326931190013634369"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Display 'y' in Brailles\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "dd = data[100][0].numpy()\n",
        "print('Braille Target: Y/y')\n",
        "plt.imshow(dd)\n",
        "plt.show()\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Braille Target: Y/y\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f62bd6e95c0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "n7kpWYqcx0Ab",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CNN Model"
      ]
    },
    {
      "metadata": {
        "id": "zjZ3lalDfUrv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the Model"
      ]
    },
    {
      "metadata": {
        "id": "ruhzBcNumGl0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "76d82935-f881-4ccc-ff69-b1621dbbf381",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528300360762,
          "user_tz": 240,
          "elapsed": 317,
          "user": {
            "displayName": "Ting-Yi Su",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118326931190013634369"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# import the nn.Module class\n",
        "import torch.nn as nn\n",
        "\n",
        "# defines the convolutional neural network\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.block1 = nn.Sequential(\n",
        "            #3x28x28\n",
        "            nn.Conv2d(in_channels=3, \n",
        "                      out_channels=16, \n",
        "                      kernel_size=5, \n",
        "                      stride=1, \n",
        "                      padding=2),\n",
        "            #16x28x28\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "            #16x14x14\n",
        "        )\n",
        "        #16x14x14\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, \n",
        "                      out_channels=32, \n",
        "                      kernel_size=5, \n",
        "                      stride=1, \n",
        "                      padding=2),\n",
        "            #32x14x14\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "            #32x7x7\n",
        "        ) \n",
        "        # linearly \n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Linear(32*7*7, 500),\n",
        "            nn.Linear(500, 300),\n",
        "            nn.Linear(300, 100),\n",
        "            nn.Linear(100, 26)\n",
        "        )\n",
        "        \n",
        "        #1x26\n",
        "    \n",
        "    def forward(self, x): \n",
        "        out = self.block1(x)\n",
        "        out = self.block2(out)\n",
        "        # flatten the dataset\n",
        "        out = out.view(-1, 32*7*7)\n",
        "        out = self.block3(out)\n",
        "        \n",
        "        return out\n",
        "\n",
        "# convolutional neural network model\n",
        "model = CNN()\n",
        "\n",
        "# if using GPU\n",
        "if use_gpu:\n",
        "  # switch model to GPU\n",
        "  model.cuda()\n",
        "\n",
        "# print summary of the neural network model to check if everything is fine. \n",
        "print(model)\n",
        "print(\"# parameter: \", sum([param.nelement() for param in model.parameters()]))\n",
        "print(image.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN(\n",
            "  (block1): Sequential(\n",
            "    (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (block2): Sequential(\n",
            "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (block3): Sequential(\n",
            "    (0): Linear(in_features=1568, out_features=500, bias=True)\n",
            "    (1): Linear(in_features=500, out_features=300, bias=True)\n",
            "    (2): Linear(in_features=300, out_features=100, bias=True)\n",
            "    (3): Linear(in_features=100, out_features=26, bias=True)\n",
            "  )\n",
            ")\n",
            "# parameter:  981574\n",
            "torch.Size([28, 28, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "broXSAfX9fma",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Set the learning rate, criterion, & optimizer"
      ]
    },
    {
      "metadata": {
        "id": "0qiBVs22kt0S",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#setting the learning rate\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Using a variable to store the cross entropy method\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Using a variable to store the optimizer \n",
        "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CLfmDuBL9lqi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train and Evaluate the data"
      ]
    },
    {
      "metadata": {
        "id": "kvfb94yR9XPK",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 12482
        },
        "outputId": "0303ccfb-c36e-4184-cefd-89d664db035a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528301274231,
          "user_tz": 240,
          "elapsed": 11838,
          "user": {
            "displayName": "Ting-Yi Su",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118326931190013634369"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "t0 = time.time()\n",
        "\n",
        "# variable to store the total loss\n",
        "total_loss = []\n",
        "\n",
        "# for loop that iterates over all the epochs\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    # variables to store/keep track of the loss and number of iterations\n",
        "    train_loss = 0\n",
        "    num_iter = 0\n",
        "    \n",
        "    # train the model\n",
        "    model.train()\n",
        "    \n",
        "    # Iterate over data.\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "      \n",
        "       \n",
        "        # print(images.shape)\n",
        "        # print(labels)\n",
        "        \n",
        "        # need to permute so that the images are of size 3x28x28 \n",
        "        # essential to be able to feed images into the model\n",
        "        images = images.permute(0, 3, 1, 2)\n",
        "        # print(images.shape)\n",
        "\n",
        "        # if GPU is available \n",
        "        if use_gpu:\n",
        "          # switch tensor type to GPU\n",
        "          images = images.cuda()\n",
        "          labels = labels.cuda()\n",
        "\n",
        "        # Zero the gradient buffer\n",
        "        # resets the gradient after each epoch so that the gradients don't add up\n",
        "        optimizer.zero_grad()  \n",
        "        \n",
        "        # Forward\n",
        "        outputs = model(images)\n",
        "       \n",
        "        # calculate the loss\n",
        "        loss = criterion(outputs, labels.view(-1))\n",
        "        print('loss:', loss)\n",
        "        total_loss.append(loss)\n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        \n",
        "        # Optimize\n",
        "        # loops through all parameters and updates weights by using the gradients \n",
        "        optimizer.step()\n",
        "        # update the training loss and number of iterations\n",
        "        train_loss += loss.data[0]\n",
        "        num_iter += 1\n",
        "    \n",
        "    print('Epoch: {}, Loss: {:.4f}'.format(\n",
        "          epoch+1, train_loss/num_iter))\n",
        "    \n",
        "    # evaluate the model\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Iterate over data.\n",
        "    for images, labels in test_loader:  \n",
        "       \n",
        "       # need to permute so that the images are of size 3x28x28 \n",
        "       # essential to be able to feed images into the model\n",
        "       images = images.permute(0, 3, 1, 2)\n",
        "      \n",
        "       # if GPU is available \n",
        "       if use_gpu:\n",
        "          # switch tensor type to GPU\n",
        "          images = images.cuda()\n",
        "          labels = labels.cuda()\n",
        "          \n",
        "       # Forward\n",
        "       outputs = model(images)\n",
        "       loss = criterion(outputs, labels.view(-1))\n",
        "       _, predicted = torch.max(outputs.data, 1)\n",
        "    \n",
        "       # Statistics\n",
        "       total += labels.size(0)\n",
        "       correct += (predicted == labels).sum()\n",
        "       \n",
        "    print('Accuracy on the test set: {}%'.format(100 * correct / total))\n",
        "\n",
        "tf = time.time()\n",
        "print()\n",
        "print(\"time: {} s\" .format(tf-t0))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: tensor(3.3086)\n",
            "loss: tensor(3.2081)\n",
            "loss: tensor(3.3087)\n",
            "loss: tensor(3.2799)\n",
            "loss: tensor(3.2736)\n",
            "loss: tensor(3.2189)\n",
            "loss: tensor(3.1845)\n",
            "loss: tensor(3.2326)\n",
            "loss: tensor(3.2542)\n",
            "loss: tensor(3.2586)\n",
            "loss: tensor(3.1712)\n",
            "loss: tensor(3.2626)\n",
            "loss: tensor(3.2449)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: tensor(3.2851)\n",
            "loss: tensor(3.2101)\n",
            "loss: tensor(3.2556)\n",
            "loss: tensor(3.1874)\n",
            "loss: tensor(3.2446)\n",
            "loss: tensor(3.2638)\n",
            "loss: tensor(3.2355)\n",
            "loss: tensor(3.2758)\n",
            "loss: tensor(3.2263)\n",
            "loss: tensor(3.2432)\n",
            "loss: tensor(3.2193)\n",
            "loss: tensor(3.2580)\n",
            "loss: tensor(3.2226)\n",
            "loss: tensor(3.3182)\n",
            "loss: tensor(3.2527)\n",
            "loss: tensor(3.3379)\n",
            "loss: tensor(3.2245)\n",
            "loss: tensor(3.2753)\n",
            "loss: tensor(3.2224)\n",
            "loss: tensor(3.2677)\n",
            "loss: tensor(3.1523)\n",
            "loss: tensor(3.2356)\n",
            "loss: tensor(3.2469)\n",
            "loss: tensor(3.2639)\n",
            "loss: tensor(3.2610)\n",
            "loss: tensor(3.2490)\n",
            "loss: tensor(3.2249)\n",
            "loss: tensor(3.2152)\n",
            "loss: tensor(3.2739)\n",
            "loss: tensor(3.2166)\n",
            "loss: tensor(3.2573)\n",
            "loss: tensor(3.2536)\n",
            "loss: tensor(3.2258)\n",
            "loss: tensor(3.2732)\n",
            "loss: tensor(3.2746)\n",
            "loss: tensor(3.2534)\n",
            "loss: tensor(3.3122)\n",
            "loss: tensor(3.2301)\n",
            "loss: tensor(3.2111)\n",
            "loss: tensor(3.2470)\n",
            "loss: tensor(3.2661)\n",
            "loss: tensor(3.2415)\n",
            "loss: tensor(3.2051)\n",
            "loss: tensor(3.2035)\n",
            "loss: tensor(3.2268)\n",
            "loss: tensor(3.1981)\n",
            "loss: tensor(3.2312)\n",
            "loss: tensor(3.3058)\n",
            "loss: tensor(3.2787)\n",
            "loss: tensor(3.2469)\n",
            "loss: tensor(3.2523)\n",
            "loss: tensor(3.2686)\n",
            "loss: tensor(3.2021)\n",
            "loss: tensor(3.3110)\n",
            "loss: tensor(3.2357)\n",
            "loss: tensor(3.3316)\n",
            "Epoch: 1, Loss: 3.2480\n",
            "Accuracy on the test set: 19%\n",
            "loss: tensor(3.2306)\n",
            "loss: tensor(3.2645)\n",
            "loss: tensor(3.2885)\n",
            "loss: tensor(3.2091)\n",
            "loss: tensor(3.2059)\n",
            "loss: tensor(3.2857)\n",
            "loss: tensor(3.2650)\n",
            "loss: tensor(3.3029)\n",
            "loss: tensor(3.2451)\n",
            "loss: tensor(3.2393)\n",
            "loss: tensor(3.2555)\n",
            "loss: tensor(3.2962)\n",
            "loss: tensor(3.2290)\n",
            "loss: tensor(3.1733)\n",
            "loss: tensor(3.2483)\n",
            "loss: tensor(3.2352)\n",
            "loss: tensor(3.3187)\n",
            "loss: tensor(3.2261)\n",
            "loss: tensor(3.2230)\n",
            "loss: tensor(3.2819)\n",
            "loss: tensor(3.2654)\n",
            "loss: tensor(3.1665)\n",
            "loss: tensor(3.2611)\n",
            "loss: tensor(3.2548)\n",
            "loss: tensor(3.2826)\n",
            "loss: tensor(3.2553)\n",
            "loss: tensor(3.2672)\n",
            "loss: tensor(3.2723)\n",
            "loss: tensor(3.2019)\n",
            "loss: tensor(3.2625)\n",
            "loss: tensor(3.2555)\n",
            "loss: tensor(3.2553)\n",
            "loss: tensor(3.2824)\n",
            "loss: tensor(3.2391)\n",
            "loss: tensor(3.2482)\n",
            "loss: tensor(3.2100)\n",
            "loss: tensor(3.2381)\n",
            "loss: tensor(3.2941)\n",
            "loss: tensor(3.2577)\n",
            "loss: tensor(3.1556)\n",
            "loss: tensor(3.2131)\n",
            "loss: tensor(3.2995)\n",
            "loss: tensor(3.3037)\n",
            "loss: tensor(3.2869)\n",
            "loss: tensor(3.2602)\n",
            "loss: tensor(3.2580)\n",
            "loss: tensor(3.2450)\n",
            "loss: tensor(3.2942)\n",
            "loss: tensor(3.2658)\n",
            "loss: tensor(3.2260)\n",
            "loss: tensor(3.1735)\n",
            "loss: tensor(3.2332)\n",
            "loss: tensor(3.2014)\n",
            "loss: tensor(3.2314)\n",
            "loss: tensor(3.1980)\n",
            "loss: tensor(3.2589)\n",
            "loss: tensor(3.2564)\n",
            "loss: tensor(3.2812)\n",
            "loss: tensor(3.2716)\n",
            "loss: tensor(3.2068)\n",
            "loss: tensor(3.2181)\n",
            "loss: tensor(3.2892)\n",
            "loss: tensor(3.2965)\n",
            "loss: tensor(3.2296)\n",
            "loss: tensor(3.2116)\n",
            "loss: tensor(3.2073)\n",
            "loss: tensor(3.2330)\n",
            "loss: tensor(3.2334)\n",
            "loss: tensor(3.1964)\n",
            "Epoch: 2, Loss: 3.2468\n",
            "Accuracy on the test set: 19%\n",
            "loss: tensor(3.2274)\n",
            "loss: tensor(3.2755)\n",
            "loss: tensor(3.2401)\n",
            "loss: tensor(3.2704)\n",
            "loss: tensor(3.2827)\n",
            "loss: tensor(3.2194)\n",
            "loss: tensor(3.2073)\n",
            "loss: tensor(3.2483)\n",
            "loss: tensor(3.2760)\n",
            "loss: tensor(3.2568)\n",
            "loss: tensor(3.2126)\n",
            "loss: tensor(3.1986)\n",
            "loss: tensor(3.2194)\n",
            "loss: tensor(3.2161)\n",
            "loss: tensor(3.2447)\n",
            "loss: tensor(3.2612)\n",
            "loss: tensor(3.2925)\n",
            "loss: tensor(3.2587)\n",
            "loss: tensor(3.2752)\n",
            "loss: tensor(3.2597)\n",
            "loss: tensor(3.2608)\n",
            "loss: tensor(3.2832)\n",
            "loss: tensor(3.2600)\n",
            "loss: tensor(3.2985)\n",
            "loss: tensor(3.2030)\n",
            "loss: tensor(3.2454)\n",
            "loss: tensor(3.2462)\n",
            "loss: tensor(3.2611)\n",
            "loss: tensor(3.2072)\n",
            "loss: tensor(3.2341)\n",
            "loss: tensor(3.2112)\n",
            "loss: tensor(3.3025)\n",
            "loss: tensor(3.2466)\n",
            "loss: tensor(3.3110)\n",
            "loss: tensor(3.2325)\n",
            "loss: tensor(3.2440)\n",
            "loss: tensor(3.3421)\n",
            "loss: tensor(3.2734)\n",
            "loss: tensor(3.2171)\n",
            "loss: tensor(3.1975)\n",
            "loss: tensor(3.2132)\n",
            "loss: tensor(3.2307)\n",
            "loss: tensor(3.1895)\n",
            "loss: tensor(3.2392)\n",
            "loss: tensor(3.2386)\n",
            "loss: tensor(3.2334)\n",
            "loss: tensor(3.2797)\n",
            "loss: tensor(3.2198)\n",
            "loss: tensor(3.2509)\n",
            "loss: tensor(3.2805)\n",
            "loss: tensor(3.2072)\n",
            "loss: tensor(3.2725)\n",
            "loss: tensor(3.2665)\n",
            "loss: tensor(3.2492)\n",
            "loss: tensor(3.2203)\n",
            "loss: tensor(3.2428)\n",
            "loss: tensor(3.2332)\n",
            "loss: tensor(3.2322)\n",
            "loss: tensor(3.2019)\n",
            "loss: tensor(3.2982)\n",
            "loss: tensor(3.2604)\n",
            "loss: tensor(3.2334)\n",
            "loss: tensor(3.2343)\n",
            "loss: tensor(3.1977)\n",
            "loss: tensor(3.3008)\n",
            "loss: tensor(3.2328)\n",
            "loss: tensor(3.2078)\n",
            "loss: tensor(3.2619)\n",
            "loss: tensor(3.3701)\n",
            "Epoch: 3, Loss: 3.2481\n",
            "Accuracy on the test set: 19%\n",
            "loss: tensor(3.1892)\n",
            "loss: tensor(3.3022)\n",
            "loss: tensor(3.2107)\n",
            "loss: tensor(3.1960)\n",
            "loss: tensor(3.2466)\n",
            "loss: tensor(3.2851)\n",
            "loss: tensor(3.1613)\n",
            "loss: tensor(3.2715)\n",
            "loss: tensor(3.2772)\n",
            "loss: tensor(3.2275)\n",
            "loss: tensor(3.2550)\n",
            "loss: tensor(3.2058)\n",
            "loss: tensor(3.1761)\n",
            "loss: tensor(3.2071)\n",
            "loss: tensor(3.3247)\n",
            "loss: tensor(3.2140)\n",
            "loss: tensor(3.2691)\n",
            "loss: tensor(3.2696)\n",
            "loss: tensor(3.2380)\n",
            "loss: tensor(3.2083)\n",
            "loss: tensor(3.2969)\n",
            "loss: tensor(3.2429)\n",
            "loss: tensor(3.2539)\n",
            "loss: tensor(3.2868)\n",
            "loss: tensor(3.2285)\n",
            "loss: tensor(3.3132)\n",
            "loss: tensor(3.2544)\n",
            "loss: tensor(3.2554)\n",
            "loss: tensor(3.2378)\n",
            "loss: tensor(3.2615)\n",
            "loss: tensor(3.1819)\n",
            "loss: tensor(3.2238)\n",
            "loss: tensor(3.2987)\n",
            "loss: tensor(3.2684)\n",
            "loss: tensor(3.2266)\n",
            "loss: tensor(3.2579)\n",
            "loss: tensor(3.2434)\n",
            "loss: tensor(3.2423)\n",
            "loss: tensor(3.3045)\n",
            "loss: tensor(3.2338)\n",
            "loss: tensor(3.1897)\n",
            "loss: tensor(3.2636)\n",
            "loss: tensor(3.2604)\n",
            "loss: tensor(3.2481)\n",
            "loss: tensor(3.2506)\n",
            "loss: tensor(3.2679)\n",
            "loss: tensor(3.2338)\n",
            "loss: tensor(3.2297)\n",
            "loss: tensor(3.2447)\n",
            "loss: tensor(3.1892)\n",
            "loss: tensor(3.2271)\n",
            "loss: tensor(3.2056)\n",
            "loss: tensor(3.1978)\n",
            "loss: tensor(3.2165)\n",
            "loss: tensor(3.3170)\n",
            "loss: tensor(3.2565)\n",
            "loss: tensor(3.2692)\n",
            "loss: tensor(3.2716)\n",
            "loss: tensor(3.2471)\n",
            "loss: tensor(3.3037)\n",
            "loss: tensor(3.2639)\n",
            "loss: tensor(3.2880)\n",
            "loss: tensor(3.2223)\n",
            "loss: tensor(3.2622)\n",
            "loss: tensor(3.2864)\n",
            "loss: tensor(3.2090)\n",
            "loss: tensor(3.2037)\n",
            "loss: tensor(3.3237)\n",
            "loss: tensor(3.2379)\n",
            "Epoch: 4, Loss: 3.2469\n",
            "Accuracy on the test set: 19%\n",
            "loss: tensor(3.2082)\n",
            "loss: tensor(3.2373)\n",
            "loss: tensor(3.2641)\n",
            "loss: tensor(3.2901)\n",
            "loss: tensor(3.2622)\n",
            "loss: tensor(3.2348)\n",
            "loss: tensor(3.2636)\n",
            "loss: tensor(3.1986)\n",
            "loss: tensor(3.2672)\n",
            "loss: tensor(3.2954)\n",
            "loss: tensor(3.2391)\n",
            "loss: tensor(3.1671)\n",
            "loss: tensor(3.2526)\n",
            "loss: tensor(3.2010)\n",
            "loss: tensor(3.2521)\n",
            "loss: tensor(3.2003)\n",
            "loss: tensor(3.2423)\n",
            "loss: tensor(3.2901)\n",
            "loss: tensor(3.2318)\n",
            "loss: tensor(3.2424)\n",
            "loss: tensor(3.2390)\n",
            "loss: tensor(3.2357)\n",
            "loss: tensor(3.2682)\n",
            "loss: tensor(3.2446)\n",
            "loss: tensor(3.2071)\n",
            "loss: tensor(3.2081)\n",
            "loss: tensor(3.2538)\n",
            "loss: tensor(3.2903)\n",
            "loss: tensor(3.2281)\n",
            "loss: tensor(3.2149)\n",
            "loss: tensor(3.2635)\n",
            "loss: tensor(3.2880)\n",
            "loss: tensor(3.2362)\n",
            "loss: tensor(3.2005)\n",
            "loss: tensor(3.2248)\n",
            "loss: tensor(3.2452)\n",
            "loss: tensor(3.2432)\n",
            "loss: tensor(3.2540)\n",
            "loss: tensor(3.2554)\n",
            "loss: tensor(3.2753)\n",
            "loss: tensor(3.2011)\n",
            "loss: tensor(3.2423)\n",
            "loss: tensor(3.2619)\n",
            "loss: tensor(3.2166)\n",
            "loss: tensor(3.2478)\n",
            "loss: tensor(3.2441)\n",
            "loss: tensor(3.2612)\n",
            "loss: tensor(3.2435)\n",
            "loss: tensor(3.1915)\n",
            "loss: tensor(3.2421)\n",
            "loss: tensor(3.2766)\n",
            "loss: tensor(3.2603)\n",
            "loss: tensor(3.2501)\n",
            "loss: tensor(3.3000)\n",
            "loss: tensor(3.2572)\n",
            "loss: tensor(3.2102)\n",
            "loss: tensor(3.2774)\n",
            "loss: tensor(3.2739)\n",
            "loss: tensor(3.2593)\n",
            "loss: tensor(3.2901)\n",
            "loss: tensor(3.1973)\n",
            "loss: tensor(3.2316)\n",
            "loss: tensor(3.3204)\n",
            "loss: tensor(3.2447)\n",
            "loss: tensor(3.2195)\n",
            "loss: tensor(3.2979)\n",
            "loss: tensor(3.2436)\n",
            "loss: tensor(3.3163)\n",
            "loss: tensor(3.2208)\n",
            "Epoch: 5, Loss: 3.2466\n",
            "Accuracy on the test set: 19%\n",
            "loss: tensor(3.1948)\n",
            "loss: tensor(3.2115)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: tensor(3.2698)\n",
            "loss: tensor(3.2695)\n",
            "loss: tensor(3.2695)\n",
            "loss: tensor(3.2447)\n",
            "loss: tensor(3.2901)\n",
            "loss: tensor(3.2486)\n",
            "loss: tensor(3.2373)\n",
            "loss: tensor(3.2411)\n",
            "loss: tensor(3.2136)\n",
            "loss: tensor(3.2459)\n",
            "loss: tensor(3.2582)\n",
            "loss: tensor(3.2478)\n",
            "loss: tensor(3.2824)\n",
            "loss: tensor(3.3200)\n",
            "loss: tensor(3.1967)\n",
            "loss: tensor(3.2836)\n",
            "loss: tensor(3.2624)\n",
            "loss: tensor(3.2339)\n",
            "loss: tensor(3.2588)\n",
            "loss: tensor(3.1721)\n",
            "loss: tensor(3.2361)\n",
            "loss: tensor(3.3102)\n",
            "loss: tensor(3.2612)\n",
            "loss: tensor(3.2608)\n",
            "loss: tensor(3.1825)\n",
            "loss: tensor(3.2583)\n",
            "loss: tensor(3.2818)\n",
            "loss: tensor(3.2664)\n",
            "loss: tensor(3.3078)\n",
            "loss: tensor(3.2300)\n",
            "loss: tensor(3.2730)\n",
            "loss: tensor(3.2638)\n",
            "loss: tensor(3.2661)\n",
            "loss: tensor(3.2548)\n",
            "loss: tensor(3.2730)\n",
            "loss: tensor(3.2527)\n",
            "loss: tensor(3.2365)\n",
            "loss: tensor(3.1992)\n",
            "loss: tensor(3.2054)\n",
            "loss: tensor(3.1913)\n",
            "loss: tensor(3.2684)\n",
            "loss: tensor(3.1727)\n",
            "loss: tensor(3.2552)\n",
            "loss: tensor(3.2819)\n",
            "loss: tensor(3.2545)\n",
            "loss: tensor(3.2588)\n",
            "loss: tensor(3.2157)\n",
            "loss: tensor(3.2387)\n",
            "loss: tensor(3.2139)\n",
            "loss: tensor(3.2151)\n",
            "loss: tensor(3.2621)\n",
            "loss: tensor(3.2466)\n",
            "loss: tensor(3.2548)\n",
            "loss: tensor(3.2600)\n",
            "loss: tensor(3.2643)\n",
            "loss: tensor(3.2425)\n",
            "loss: tensor(3.2508)\n",
            "loss: tensor(3.2715)\n",
            "loss: tensor(3.2262)\n",
            "loss: tensor(3.2478)\n",
            "loss: tensor(3.2251)\n",
            "loss: tensor(3.2062)\n",
            "loss: tensor(3.2703)\n",
            "loss: tensor(3.2395)\n",
            "loss: tensor(3.2333)\n",
            "loss: tensor(3.2339)\n",
            "loss: tensor(3.2487)\n",
            "Epoch: 6, Loss: 3.2467\n",
            "Accuracy on the test set: 19%\n",
            "loss: tensor(3.2199)\n",
            "loss: tensor(3.3150)\n",
            "loss: tensor(3.2050)\n",
            "loss: tensor(3.2501)\n",
            "loss: tensor(3.2597)\n",
            "loss: tensor(3.1995)\n",
            "loss: tensor(3.2151)\n",
            "loss: tensor(3.2523)\n",
            "loss: tensor(3.2287)\n",
            "loss: tensor(3.2577)\n",
            "loss: tensor(3.2710)\n",
            "loss: tensor(3.2148)\n",
            "loss: tensor(3.3027)\n",
            "loss: tensor(3.2490)\n",
            "loss: tensor(3.2431)\n",
            "loss: tensor(3.2278)\n",
            "loss: tensor(3.2602)\n",
            "loss: tensor(3.2436)\n",
            "loss: tensor(3.2787)\n",
            "loss: tensor(3.2362)\n",
            "loss: tensor(3.2805)\n",
            "loss: tensor(3.3013)\n",
            "loss: tensor(3.2147)\n",
            "loss: tensor(3.2576)\n",
            "loss: tensor(3.2590)\n",
            "loss: tensor(3.2386)\n",
            "loss: tensor(3.2510)\n",
            "loss: tensor(3.2288)\n",
            "loss: tensor(3.2497)\n",
            "loss: tensor(3.2437)\n",
            "loss: tensor(3.3062)\n",
            "loss: tensor(3.2450)\n",
            "loss: tensor(3.2126)\n",
            "loss: tensor(3.1868)\n",
            "loss: tensor(3.2922)\n",
            "loss: tensor(3.2268)\n",
            "loss: tensor(3.1966)\n",
            "loss: tensor(3.2269)\n",
            "loss: tensor(3.2182)\n",
            "loss: tensor(3.2422)\n",
            "loss: tensor(3.2839)\n",
            "loss: tensor(3.1957)\n",
            "loss: tensor(3.2536)\n",
            "loss: tensor(3.2039)\n",
            "loss: tensor(3.3247)\n",
            "loss: tensor(3.2619)\n",
            "loss: tensor(3.2341)\n",
            "loss: tensor(3.2313)\n",
            "loss: tensor(3.2456)\n",
            "loss: tensor(3.2463)\n",
            "loss: tensor(3.2804)\n",
            "loss: tensor(3.2481)\n",
            "loss: tensor(3.2965)\n",
            "loss: tensor(3.1997)\n",
            "loss: tensor(3.2375)\n",
            "loss: tensor(3.2125)\n",
            "loss: tensor(3.1967)\n",
            "loss: tensor(3.2579)\n",
            "loss: tensor(3.3250)\n",
            "loss: tensor(3.2307)\n",
            "loss: tensor(3.2599)\n",
            "loss: tensor(3.2425)\n",
            "loss: tensor(3.1928)\n",
            "loss: tensor(3.2803)\n",
            "loss: tensor(3.2319)\n",
            "loss: tensor(3.2181)\n",
            "loss: tensor(3.2261)\n",
            "loss: tensor(3.3259)\n",
            "loss: tensor(3.2907)\n",
            "Epoch: 7, Loss: 3.2470\n",
            "Accuracy on the test set: 19%\n",
            "loss: tensor(3.2564)\n",
            "loss: tensor(3.2552)\n",
            "loss: tensor(3.2480)\n",
            "loss: tensor(3.2487)\n",
            "loss: tensor(3.2321)\n",
            "loss: tensor(3.2186)\n",
            "loss: tensor(3.2729)\n",
            "loss: tensor(3.2503)\n",
            "loss: tensor(3.2358)\n",
            "loss: tensor(3.2710)\n",
            "loss: tensor(3.2410)\n",
            "loss: tensor(3.2811)\n",
            "loss: tensor(3.1978)\n",
            "loss: tensor(3.2860)\n",
            "loss: tensor(3.2550)\n",
            "loss: tensor(3.2534)\n",
            "loss: tensor(3.2001)\n",
            "loss: tensor(3.2448)\n",
            "loss: tensor(3.3293)\n",
            "loss: tensor(3.2639)\n",
            "loss: tensor(3.2519)\n",
            "loss: tensor(3.2223)\n",
            "loss: tensor(3.2507)\n",
            "loss: tensor(3.2504)\n",
            "loss: tensor(3.1738)\n",
            "loss: tensor(3.2295)\n",
            "loss: tensor(3.3091)\n",
            "loss: tensor(3.2435)\n",
            "loss: tensor(3.1933)\n",
            "loss: tensor(3.2368)\n",
            "loss: tensor(3.2893)\n",
            "loss: tensor(3.2902)\n",
            "loss: tensor(3.2456)\n",
            "loss: tensor(3.2656)\n",
            "loss: tensor(3.2366)\n",
            "loss: tensor(3.2193)\n",
            "loss: tensor(3.1807)\n",
            "loss: tensor(3.1882)\n",
            "loss: tensor(3.2283)\n",
            "loss: tensor(3.2272)\n",
            "loss: tensor(3.2515)\n",
            "loss: tensor(3.2160)\n",
            "loss: tensor(3.2412)\n",
            "loss: tensor(3.2619)\n",
            "loss: tensor(3.2304)\n",
            "loss: tensor(3.2764)\n",
            "loss: tensor(3.2835)\n",
            "loss: tensor(3.2499)\n",
            "loss: tensor(3.2485)\n",
            "loss: tensor(3.1553)\n",
            "loss: tensor(3.2746)\n",
            "loss: tensor(3.2692)\n",
            "loss: tensor(3.2591)\n",
            "loss: tensor(3.2666)\n",
            "loss: tensor(3.2853)\n",
            "loss: tensor(3.2527)\n",
            "loss: tensor(3.2158)\n",
            "loss: tensor(3.2766)\n",
            "loss: tensor(3.2025)\n",
            "loss: tensor(3.2398)\n",
            "loss: tensor(3.2265)\n",
            "loss: tensor(3.2744)\n",
            "loss: tensor(3.2977)\n",
            "loss: tensor(3.2150)\n",
            "loss: tensor(3.2534)\n",
            "loss: tensor(3.2406)\n",
            "loss: tensor(3.2896)\n",
            "loss: tensor(3.2575)\n",
            "loss: tensor(3.1889)\n",
            "Epoch: 8, Loss: 3.2460\n",
            "Accuracy on the test set: 19%\n",
            "loss: tensor(3.2885)\n",
            "loss: tensor(3.2531)\n",
            "loss: tensor(3.2637)\n",
            "loss: tensor(3.2378)\n",
            "loss: tensor(3.2632)\n",
            "loss: tensor(3.2751)\n",
            "loss: tensor(3.2156)\n",
            "loss: tensor(3.2309)\n",
            "loss: tensor(3.2868)\n",
            "loss: tensor(3.2753)\n",
            "loss: tensor(3.2895)\n",
            "loss: tensor(3.2851)\n",
            "loss: tensor(3.2082)\n",
            "loss: tensor(3.2631)\n",
            "loss: tensor(3.2553)\n",
            "loss: tensor(3.1799)\n",
            "loss: tensor(3.2042)\n",
            "loss: tensor(3.2629)\n",
            "loss: tensor(3.2531)\n",
            "loss: tensor(3.2747)\n",
            "loss: tensor(3.2730)\n",
            "loss: tensor(3.2576)\n",
            "loss: tensor(3.2697)\n",
            "loss: tensor(3.2062)\n",
            "loss: tensor(3.2506)\n",
            "loss: tensor(3.1895)\n",
            "loss: tensor(3.1320)\n",
            "loss: tensor(3.2424)\n",
            "loss: tensor(3.2587)\n",
            "loss: tensor(3.2691)\n",
            "loss: tensor(3.2311)\n",
            "loss: tensor(3.2615)\n",
            "loss: tensor(3.2145)\n",
            "loss: tensor(3.1500)\n",
            "loss: tensor(3.2686)\n",
            "loss: tensor(3.2044)\n",
            "loss: tensor(3.2124)\n",
            "loss: tensor(3.2103)\n",
            "loss: tensor(3.2446)\n",
            "loss: tensor(3.2140)\n",
            "loss: tensor(3.2816)\n",
            "loss: tensor(3.2751)\n",
            "loss: tensor(3.2989)\n",
            "loss: tensor(3.3144)\n",
            "loss: tensor(3.2517)\n",
            "loss: tensor(3.2684)\n",
            "loss: tensor(3.2306)\n",
            "loss: tensor(3.2645)\n",
            "loss: tensor(3.2765)\n",
            "loss: tensor(3.2227)\n",
            "loss: tensor(3.2237)\n",
            "loss: tensor(3.2419)\n",
            "loss: tensor(3.2516)\n",
            "loss: tensor(3.2918)\n",
            "loss: tensor(3.2497)\n",
            "loss: tensor(3.2274)\n",
            "loss: tensor(3.2177)\n",
            "loss: tensor(3.2861)\n",
            "loss: tensor(3.2032)\n",
            "loss: tensor(3.1868)\n",
            "loss: tensor(3.2388)\n",
            "loss: tensor(3.2727)\n",
            "loss: tensor(3.1963)\n",
            "loss: tensor(3.2475)\n",
            "loss: tensor(3.2449)\n",
            "loss: tensor(3.2291)\n",
            "loss: tensor(3.2388)\n",
            "loss: tensor(3.3143)\n",
            "loss: tensor(3.4472)\n",
            "Epoch: 9, Loss: 3.2481\n",
            "Accuracy on the test set: 19%\n",
            "loss: tensor(3.2374)\n",
            "loss: tensor(3.2858)\n",
            "loss: tensor(3.1871)\n",
            "loss: tensor(3.2563)\n",
            "loss: tensor(3.2600)\n",
            "loss: tensor(3.2366)\n",
            "loss: tensor(3.1550)\n",
            "loss: tensor(3.2319)\n",
            "loss: tensor(3.2070)\n",
            "loss: tensor(3.2681)\n",
            "loss: tensor(3.2214)\n",
            "loss: tensor(3.2695)\n",
            "loss: tensor(3.2113)\n",
            "loss: tensor(3.2485)\n",
            "loss: tensor(3.1745)\n",
            "loss: tensor(3.2476)\n",
            "loss: tensor(3.2204)\n",
            "loss: tensor(3.2011)\n",
            "loss: tensor(3.2446)\n",
            "loss: tensor(3.2818)\n",
            "loss: tensor(3.2550)\n",
            "loss: tensor(3.2005)\n",
            "loss: tensor(3.2631)\n",
            "loss: tensor(3.2825)\n",
            "loss: tensor(3.2250)\n",
            "loss: tensor(3.2981)\n",
            "loss: tensor(3.2625)\n",
            "loss: tensor(3.2584)\n",
            "loss: tensor(3.2769)\n",
            "loss: tensor(3.2873)\n",
            "loss: tensor(3.2033)\n",
            "loss: tensor(3.2242)\n",
            "loss: tensor(3.2433)\n",
            "loss: tensor(3.1857)\n",
            "loss: tensor(3.2342)\n",
            "loss: tensor(3.2531)\n",
            "loss: tensor(3.2208)\n",
            "loss: tensor(3.2064)\n",
            "loss: tensor(3.2270)\n",
            "loss: tensor(3.2314)\n",
            "loss: tensor(3.2605)\n",
            "loss: tensor(3.2108)\n",
            "loss: tensor(3.2862)\n",
            "loss: tensor(3.2815)\n",
            "loss: tensor(3.2980)\n",
            "loss: tensor(3.2887)\n",
            "loss: tensor(3.2587)\n",
            "loss: tensor(3.2486)\n",
            "loss: tensor(3.2467)\n",
            "loss: tensor(3.3032)\n",
            "loss: tensor(3.2441)\n",
            "loss: tensor(3.2396)\n",
            "loss: tensor(3.1976)\n",
            "loss: tensor(3.2093)\n",
            "loss: tensor(3.2645)\n",
            "loss: tensor(3.2718)\n",
            "loss: tensor(3.2804)\n",
            "loss: tensor(3.3025)\n",
            "loss: tensor(3.2619)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: tensor(3.2749)\n",
            "loss: tensor(3.2473)\n",
            "loss: tensor(3.2418)\n",
            "loss: tensor(3.3104)\n",
            "loss: tensor(3.1723)\n",
            "loss: tensor(3.2391)\n",
            "loss: tensor(3.3077)\n",
            "loss: tensor(3.2575)\n",
            "loss: tensor(3.2651)\n",
            "loss: tensor(3.2159)\n",
            "Epoch: 10, Loss: 3.2460\n",
            "Accuracy on the test set: 19%\n",
            "\n",
            "time: 11.473206758499146 s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}