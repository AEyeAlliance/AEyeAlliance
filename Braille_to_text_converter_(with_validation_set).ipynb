{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Braille to text converter (with validation set)",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1k_N3U6wqaxsXsCVlrKs4qwPZipuU1Mk8",
          "timestamp": 1528305352966
        },
        {
          "file_id": "19cLVbAvTfuu0pqaadxpgyMsFxHC3nnNF",
          "timestamp": 1527465087778
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "H-ra1DTnR0S6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Install pytorch "
      ]
    },
    {
      "metadata": {
        "id": "ikmT9ejsWLrU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "cellView": "code",
        "outputId": "8c424d02-5d39-4834-fd74-3bf093b890a4",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528305630461,
          "user_tz": 240,
          "elapsed": 4586,
          "user": {
            "displayName": "Ting-Yi Su",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118326931190013634369"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# use cross entropy for classification problems\n",
        "\n",
        "import time\n",
        "import platform\n",
        "import io\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from matplotlib.pyplot import cm \n",
        "\n",
        "# import KFold from scikit-learn\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def install_pytorch():\n",
        "    os = platform.system()\n",
        "    if os == \"Linux\":\n",
        "        !pip3 install http://download.pytorch.org/whl/cu90/torch-0.4.0-cp36-cp36m-linux_x86_64.whl\n",
        "    elif os == \"Windows\":\n",
        "        !pip3 install http://download.pytorch.org/whl/cu90/torch-0.4.0-cp36-cp36m-win_amd64.whl \n",
        "    !pip3 install torchvision\n",
        "\n",
        "\n",
        "# Install PyTorch.\n",
        "install_pytorch()\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==0.4.0 from http://download.pytorch.org/whl/cu90/torch-0.4.0-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TuhFUrX0O2Se",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ]
    },
    {
      "metadata": {
        "id": "Tw-iCQFfRhS7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import and read the CSV file \n",
        "\n",
        "Load the dataset."
      ]
    },
    {
      "metadata": {
        "id": "_1kJYrn79Cft",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#import csv dataset\n",
        "import torchvision\n",
        "import csv\n",
        "import os\n",
        "import pandas as pd \n",
        "from urllib import request\n",
        "import requests\n",
        "\n",
        "# Upload and read the csv file from the github repo\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/HelenG123/aeye-alliance/master/Data/data_day3.csv\")\n",
        "df_test = pd.read_csv(\"https://raw.githubusercontent.com/HelenG123/ai-alliance/master/brailleFinalv2.csv\")\n",
        "\n",
        "#https://raw.githubusercontent.com/HelenG123/ai-alliance/master/braille_data.csv\n",
        "# # Read the CSV file from a local directory\n",
        "# dataset_name = list(dataset.keys())[0]\n",
        "# df = pd.read_csv(io.StringIO(dataset[dataset_name].decode('utf-8')))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NGxuIdjcd4-p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generate the Targets\n",
        "Create a dictionary that contains the target number for each image in the Braille alphabet.\n"
      ]
    },
    {
      "metadata": {
        "id": "dbQyhtDHnB6o",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a8b75638-db84-47fc-a56d-13ee1e195a7e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528307451281,
          "user_tz": 240,
          "elapsed": 285,
          "user": {
            "displayName": "Ting-Yi Su",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118326931190013634369"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# not using one hot encoding vector\n",
        "# using 1-26 to represent a-z instead\n",
        "\n",
        "import string\n",
        "\n",
        "target = {}\n",
        "alphabet = list(string.ascii_lowercase)\n",
        "\n",
        "number = 0 \n",
        "for letter in alphabet: \n",
        "  target[letter] = number\n",
        "  number += 1\n",
        "\n",
        "print(target)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TLSF1BrXd5sU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# import string\n",
        "# alphabet = list(string.ascii_lowercase)\n",
        "\n",
        "# target = {}\n",
        "\n",
        "# # Initalize a target dict that has the letters as its keys and as its value\n",
        "# # an empty one-hot encoding of size 26\n",
        "# for letter in alphabet: \n",
        "#   target[letter] = [0] * 26\n",
        "\n",
        "# # Do the one-hot encoding for each letter now \n",
        "# curr_pos = 0 \n",
        "# for curr_letter in target.keys():\n",
        "#   target[curr_letter][curr_pos] = 1\n",
        "#   curr_pos += 1  \n",
        "\n",
        "# print(target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S6XGJhcJOvCM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Iterate over the CSV files to add the targets\n",
        "Create a dictionary of the images that contains the image as a Tensor and its target as a number between 1-26."
      ]
    },
    {
      "metadata": {
        "id": "k6W-EK9_Oz_C",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 924
        },
        "outputId": "ffbf1655-0daa-4fb0-8628-94a2bf34e56a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528307547441,
          "user_tz": 240,
          "elapsed": 93309,
          "user": {
            "displayName": "Ting-Yi Su",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118326931190013634369"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from scipy import misc\n",
        "from io import BytesIO\n",
        "import urllib\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "data=[]\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "  picture = []\n",
        "  url = row['Labeled Data']\n",
        "  label = row['Label']\n",
        "  curr_target = target[label[10]]\n",
        "  # print(curr_target)\n",
        "\n",
        "  x = urllib.request.urlopen(url)\n",
        "  resp = x.read()\n",
        "  image = np.array(bytearray(resp), dtype=np.uint8)\n",
        "  image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "  # resize image to 28x28x3\n",
        "  image = cv2.resize(image, (28, 28))\n",
        "  image = image.astype(np.float32)/255.0\n",
        "   #image = image.flatten().astype(np.float32)/255.0\n",
        "  image = torch.from_numpy(image)\n",
        "  picture.append(image)\n",
        "  curr_target=torch.LongTensor([curr_target])\n",
        "  picture.append(curr_target)\n",
        "  data.append(picture)\n",
        "\n",
        "print(image.shape) # these are the dimensions of our image\n",
        "print(data[0][0])\n",
        "print(data[0][1])\n",
        "print(max([d[1] for d in data]))\n",
        "\n",
        "data_test=[]\n",
        "\n",
        "for i, row in df_test.iterrows():\n",
        "  picture = []\n",
        "  url = row['Labeled Data']\n",
        "  label = row['External ID']\n",
        "  curr_target = target[label[0]]\n",
        "  # print(curr_target)\n",
        "\n",
        "  x = urllib.request.urlopen(url)\n",
        "  resp = x.read()\n",
        "  image = np.array(bytearray(resp), dtype=np.uint8)\n",
        "  image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "  # resize image to 28x28x3\n",
        "  image = cv2.resize(image, (28, 28))\n",
        "  image = image.astype(np.float32)/255.0\n",
        "   #image = image.flatten().astype(np.float32)/255.0\n",
        "  image = torch.from_numpy(image)\n",
        "  picture.append(image)\n",
        "  curr_target=torch.LongTensor([curr_target])\n",
        "  picture.append(curr_target)\n",
        "  data_test.append(picture)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([28, 28, 3])\n",
            "tensor([[[ 0.8745,  0.8314,  0.8157],\n",
            "         [ 0.8745,  0.8275,  0.8157],\n",
            "         [ 0.8745,  0.8275,  0.8196],\n",
            "         ...,\n",
            "         [ 0.8784,  0.8314,  0.8157],\n",
            "         [ 0.8745,  0.8314,  0.8196],\n",
            "         [ 0.8745,  0.8314,  0.8196]],\n",
            "\n",
            "        [[ 0.8706,  0.8275,  0.8118],\n",
            "         [ 0.8706,  0.8275,  0.8118],\n",
            "         [ 0.8667,  0.8235,  0.8078],\n",
            "         ...,\n",
            "         [ 0.8824,  0.8353,  0.8196],\n",
            "         [ 0.8745,  0.8275,  0.8196],\n",
            "         [ 0.8706,  0.8235,  0.8157]],\n",
            "\n",
            "        [[ 0.8745,  0.8314,  0.8157],\n",
            "         [ 0.8745,  0.8314,  0.8157],\n",
            "         [ 0.8706,  0.8275,  0.8118],\n",
            "         ...,\n",
            "         [ 0.8863,  0.8392,  0.8275],\n",
            "         [ 0.8745,  0.8275,  0.8196],\n",
            "         [ 0.8745,  0.8275,  0.8196]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.8706,  0.8314,  0.8039],\n",
            "         [ 0.8824,  0.8431,  0.8157],\n",
            "         [ 0.8784,  0.8392,  0.8118],\n",
            "         ...,\n",
            "         [ 0.8706,  0.8314,  0.8039],\n",
            "         [ 0.8627,  0.8275,  0.8000],\n",
            "         [ 0.8627,  0.8275,  0.7961]],\n",
            "\n",
            "        [[ 0.8627,  0.8275,  0.7922],\n",
            "         [ 0.8745,  0.8392,  0.8039],\n",
            "         [ 0.8745,  0.8392,  0.8039],\n",
            "         ...,\n",
            "         [ 0.8784,  0.8392,  0.8118],\n",
            "         [ 0.8627,  0.8235,  0.7961],\n",
            "         [ 0.8588,  0.8196,  0.7922]],\n",
            "\n",
            "        [[ 0.8667,  0.8314,  0.7961],\n",
            "         [ 0.8745,  0.8392,  0.8039],\n",
            "         [ 0.8745,  0.8392,  0.8039],\n",
            "         ...,\n",
            "         [ 0.8667,  0.8275,  0.7961],\n",
            "         [ 0.8667,  0.8275,  0.8000],\n",
            "         [ 0.8667,  0.8275,  0.8000]]])\n",
            "tensor([ 0])\n",
            "tensor([ 25])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "thE4Nl9nMP0d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create DataLoader objects\n",
        "\n",
        "Dataloader gives the object that we can iterate over for enumerating and training our data."
      ]
    },
    {
      "metadata": {
        "id": "jAGps6lEMXVC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "927fd72f-e50f-4db7-de56-86263e0691cd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528308412360,
          "user_tz": 240,
          "elapsed": 356,
          "user": {
            "displayName": "Ting-Yi Su",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118326931190013634369"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# batch sizes for train, test, and validation\n",
        "batch_size_train = 10\n",
        "batch_size_test = 5\n",
        "batch_size_validation = 10\n",
        "\n",
        "# splitting data to get training and validation sets\n",
        "train_dataset = data[:600]\n",
        "validation_dataset = data[601:]\n",
        "\n",
        "# create the dataloader objects\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size_train, shuffle=True)\n",
        "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=batch_size_validation, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=data_test, batch_size=batch_size_test, shuffle=False)\n",
        "\n",
        "print(len(train_loader))\n",
        "print(len(validation_loader))\n",
        "print(len(test_loader))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60\n",
            "9\n",
            "16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zUxdGGHGeajB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize the Image\n",
        "Demonstrate that we can access and display an image from the dataset. \n"
      ]
    },
    {
      "metadata": {
        "id": "UkvrVL6EaJ0Z",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "c1e75192-b603-47bc-e5c9-ff29a7471e7f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528300357090,
          "user_tz": 240,
          "elapsed": 443,
          "user": {
            "displayName": "Ting-Yi Su",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118326931190013634369"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Display 'y' in Brailles\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "dd = data[100][0].numpy()\n",
        "print('Braille Target: Y/y')\n",
        "plt.imshow(dd)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Braille Target: Y/y\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f62bd6e95c0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "n7kpWYqcx0Ab",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CNN Model"
      ]
    },
    {
      "metadata": {
        "id": "zjZ3lalDfUrv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the Model"
      ]
    },
    {
      "metadata": {
        "id": "ruhzBcNumGl0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "610cdeb2-d38c-4952-f2e7-c588e97c3ec8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528307657118,
          "user_tz": 240,
          "elapsed": 357,
          "user": {
            "displayName": "Ting-Yi Su",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118326931190013634369"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# import the nn.Module class\n",
        "import torch.nn as nn\n",
        "\n",
        "# defines the convolutional neural network\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.block1 = nn.Sequential(\n",
        "            #3x28x28\n",
        "            nn.Conv2d(in_channels=3, \n",
        "                      out_channels=16, \n",
        "                      kernel_size=5, \n",
        "                      stride=1, \n",
        "                      padding=2),\n",
        "            #16x28x28\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "            #16x14x14\n",
        "        )\n",
        "        #16x14x14\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, \n",
        "                      out_channels=32, \n",
        "                      kernel_size=5, \n",
        "                      stride=1, \n",
        "                      padding=2),\n",
        "            #32x14x14\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "            #32x7x7\n",
        "        ) \n",
        "        # linearly \n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Linear(32*7*7, 500),\n",
        "            nn.Linear(500, 300),\n",
        "            nn.Linear(300, 100),\n",
        "            nn.Linear(100, 26)\n",
        "        )\n",
        "        \n",
        "        #1x26\n",
        "    \n",
        "    def forward(self, x): \n",
        "        out = self.block1(x)\n",
        "        out = self.block2(out)\n",
        "        # flatten the dataset\n",
        "        out = out.view(-1, 32*7*7)\n",
        "        out = self.block3(out)\n",
        "        \n",
        "        return out\n",
        "\n",
        "# convolutional neural network model\n",
        "model = CNN()\n",
        "\n",
        "# print summary of the neural network model to check if everything is fine. \n",
        "print(model)\n",
        "print(\"# parameter: \", sum([param.nelement() for param in model.parameters()]))\n",
        "print(image.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN(\n",
            "  (block1): Sequential(\n",
            "    (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (block2): Sequential(\n",
            "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (block3): Sequential(\n",
            "    (0): Linear(in_features=1568, out_features=500, bias=True)\n",
            "    (1): Linear(in_features=500, out_features=300, bias=True)\n",
            "    (2): Linear(in_features=300, out_features=100, bias=True)\n",
            "    (3): Linear(in_features=100, out_features=26, bias=True)\n",
            "  )\n",
            ")\n",
            "# parameter:  981574\n",
            "torch.Size([28, 28, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "broXSAfX9fma",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Set the learning rate, criterion, & optimizer"
      ]
    },
    {
      "metadata": {
        "id": "0qiBVs22kt0S",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#setting the learning rate\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Using a variable to store the cross entropy method\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Using a variable to store the optimizer \n",
        "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CLfmDuBL9lqi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train and Evaluate the data"
      ]
    },
    {
      "metadata": {
        "id": "kvfb94yR9XPK",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "outputId": "57f950d4-2b53-486c-bafd-71b81042b923",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528308425306,
          "user_tz": 240,
          "elapsed": 8642,
          "user": {
            "displayName": "Ting-Yi Su",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118326931190013634369"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "t0 = time.time()\n",
        "\n",
        "# variable to store the total loss\n",
        "total_loss = []\n",
        "\n",
        "# for loop that iterates over all the epochs\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    # variables to store/keep track of the loss and number of iterations\n",
        "    train_loss = 0\n",
        "    num_iter = 0\n",
        "    \n",
        "    # train the model\n",
        "    model.train()\n",
        "    \n",
        "    # Iterate over train_loader\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "      \n",
        "        # print(images.shape)\n",
        "        # print(labels)\n",
        "        \n",
        "        # need to permute so that the images are of size 3x28x28 \n",
        "        # essential to be able to feed images into the model\n",
        "        images = images.permute(0, 3, 1, 2)\n",
        "        # print(images.shape)\n",
        "\n",
        "        # Zero the gradient buffer\n",
        "        # resets the gradient after each epoch so that the gradients don't add up\n",
        "        optimizer.zero_grad()  \n",
        "        \n",
        "        # Forward\n",
        "        outputs = model(images)\n",
        "       \n",
        "        # calculate the loss\n",
        "        loss = criterion(outputs, labels.view(-1))\n",
        "        #print('loss:', loss)\n",
        "        total_loss.append(loss)\n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        \n",
        "        # Optimize\n",
        "        # loops through all parameters and updates weights by using the gradients \n",
        "        optimizer.step()\n",
        "        # update the training loss and number of iterations\n",
        "        train_loss += loss.data[0]\n",
        "        num_iter += 1\n",
        "    \n",
        "    print('Epoch: {}, Loss: {:.4f}'.format(\n",
        "          epoch+1, train_loss/num_iter))\n",
        "    \n",
        "    # evaluate the model\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Iterate over validation_loader\n",
        "    for images, labels in validation_loader:  \n",
        "       \n",
        "       # need to permute so that the images are of size 3x28x28 \n",
        "       # essential to be able to feed images into the model\n",
        "       images = images.permute(0, 3, 1, 2)\n",
        "          \n",
        "       # Forward\n",
        "       outputs = model(images)\n",
        "       loss = criterion(outputs, labels.view(-1))\n",
        "       _, predicted = torch.max(outputs.data, 1)\n",
        "    \n",
        "       # Statistics\n",
        "       total += labels.size(0)\n",
        "       correct += (predicted == labels).sum()\n",
        "       \n",
        "    print('Accuracy on the validation set: {}%'.format(100 * correct / total))\n",
        "\n",
        "tf = time.time()\n",
        "print()\n",
        "print(\"time: {} s\" .format(tf-t0))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 3.2475\n",
            "Accuracy on the validation set: 12%\n",
            "Epoch: 2, Loss: 3.2474\n",
            "Accuracy on the validation set: 12%\n",
            "Epoch: 3, Loss: 3.2472\n",
            "Accuracy on the validation set: 12%\n",
            "Epoch: 4, Loss: 3.2470\n",
            "Accuracy on the validation set: 12%\n",
            "Epoch: 5, Loss: 3.2469\n",
            "Accuracy on the validation set: 12%\n",
            "Epoch: 6, Loss: 3.2468\n",
            "Accuracy on the validation set: 12%\n",
            "Epoch: 7, Loss: 3.2466\n",
            "Accuracy on the validation set: 12%\n",
            "Epoch: 8, Loss: 3.2465\n",
            "Accuracy on the validation set: 3%\n",
            "Epoch: 9, Loss: 3.2464\n",
            "Accuracy on the validation set: 12%\n",
            "Epoch: 10, Loss: 3.2462\n",
            "Accuracy on the validation set: 3%\n",
            "\n",
            "time: 8.309212923049927 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B8raennyLGRj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "e9928748-0b8a-4e4c-9d40-64caac16a4d7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528308363902,
          "user_tz": 240,
          "elapsed": 760,
          "user": {
            "displayName": "Ting-Yi Su",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118326931190013634369"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "t0 = time.time()\n",
        "\n",
        "# variable to store the total loss\n",
        "total_loss = []\n",
        "\n",
        "# for loop that iterates over all the epochs\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    # evaluate the model\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Iterate over test_loader\n",
        "    for images, labels in test_loader:  \n",
        "       \n",
        "       # need to permute so that the images are of size 3x28x28 \n",
        "       # essential to be able to feed images into the model\n",
        "       images = images.permute(0, 3, 1, 2)\n",
        "          \n",
        "       # Forward\n",
        "       outputs = model(images)\n",
        "       loss = criterion(outputs, labels.view(-1))\n",
        "       _, predicted = torch.max(outputs.data, 1)\n",
        "    \n",
        "       # Statistics\n",
        "       total += labels.size(0)\n",
        "       correct += (predicted == labels).sum()\n",
        "       \n",
        "    print('Accuracy on the test set: {}%'.format(100 * correct / total))\n",
        "\n",
        "tf = time.time()\n",
        "print()\n",
        "print(\"time: {} s\" .format(tf-t0))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on the test set: 19%\n",
            "Accuracy on the test set: 19%\n",
            "Accuracy on the test set: 19%\n",
            "Accuracy on the test set: 19%\n",
            "Accuracy on the test set: 19%\n",
            "Accuracy on the test set: 19%\n",
            "Accuracy on the test set: 19%\n",
            "Accuracy on the test set: 19%\n",
            "Accuracy on the test set: 19%\n",
            "Accuracy on the test set: 19%\n",
            "\n",
            "time: 0.5491518974304199 s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}