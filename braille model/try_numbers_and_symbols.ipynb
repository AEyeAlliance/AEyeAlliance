{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to measure run-time\n",
    "import time\n",
    "\n",
    "# to shuffle data\n",
    "import random\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pytorch stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# for csv dataset\n",
    "import torchvision\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd \n",
    "from urllib import request\n",
    "import requests\n",
    "\n",
    "# import statements for iterating over csv file\n",
    "import cv2\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "\n",
    "# to get the alphabet\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload and read the csv files from the github repo\n",
    "# change once get more data\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/HelenG123/aeye-alliance/master/Labelled%20Data/symbols_letters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'b': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'c': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'd': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'e': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'f': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'g': [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'h': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'i': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'j': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'k': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'l': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'm': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'n': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'o': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'p': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'q': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'r': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 's': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 't': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'u': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'v': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'w': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'x': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'y': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'z': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], ' ': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], '#': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], '.': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], ',': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], ':': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], \"'\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], '-': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], ';': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], '?': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], '!': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], 'C': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}\n"
     ]
    }
   ],
   "source": [
    "# generate the targets \n",
    "# the targets are one hot encoding vectors\n",
    "\n",
    "alphabet = list(string.ascii_lowercase)\n",
    "target = {}\n",
    "\n",
    "# Initalize a target dict that has letters as its keys and empty one-hot encoding vectors of size 37 as its values\n",
    "for letter in alphabet: \n",
    "    target[letter] = [0] * 37\n",
    "\n",
    "# Do the one-hot encoding for each letter now \n",
    "curr_pos = 0 \n",
    "for curr_letter in target.keys():\n",
    "    target[curr_letter][curr_pos] = 1\n",
    "    curr_pos += 1  \n",
    "\n",
    "# extra symbols \n",
    "symbols = [' ', '#', '.', ',', ':', '\\'', '-', ';', '?', '!', 'C'] # C stands for CAPS\n",
    "\n",
    "# create vectors\n",
    "for curr_symbol in symbols:\n",
    "    target[curr_symbol] = [0] * 37\n",
    "\n",
    "# create one-hot encoding vectors\n",
    "for curr_symbol in symbols:\n",
    "    target[curr_symbol][curr_pos] = 1\n",
    "    curr_pos += 1\n",
    "\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 968.8291821479797 s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# collect all data from the csv file\n",
    "data=[]\n",
    "\n",
    "# iterate over csv file\n",
    "for i, row in df.iterrows():\n",
    "    # store the image and label\n",
    "    picture = []\n",
    "    url = row['Labeled Data']\n",
    "    label = row['Label']\n",
    "    curr_target = target[label[11]]\n",
    "    x = urllib.request.urlopen(url)\n",
    "    resp = x.read()\n",
    "    image = np.array(bytearray(resp), dtype=np.uint8)\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    # resize image to 28x28x3\n",
    "    image = cv2.resize(image, (28, 28))\n",
    "    # normalize to 0-1\n",
    "    image = image.astype(np.float32)/255.0\n",
    "    image = torch.from_numpy(image)\n",
    "    picture.append(image)\n",
    "    # convert the target to a long tensor\n",
    "    curr_target=torch.LongTensor([curr_target])\n",
    "    picture.append(curr_target)\n",
    "    # append the current image & target\n",
    "    data.append(picture)\n",
    "    \n",
    "tf = time.time()\n",
    "print(\"time: {} s\" .format(tf-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'a', 1: 'b', 2: 'c', 3: 'd', 4: 'e', 5: 'f', 6: 'g', 7: 'h', 8: 'i', 9: 'j', 10: 'k', 11: 'l', 12: 'm', 13: 'n', 14: 'o', 15: 'p', 16: 'q', 17: 'r', 18: 's', 19: 't', 20: 'u', 21: 'v', 22: 'w', 23: 'x', 24: 'y', 25: 'z', 26: ' ', 27: '#', 28: '.', 29: ',', 30: ':', 31: \"'\", 32: '-', 33: ';', 34: '?', 35: '!', 36: 'C'}\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary of all the characters \n",
    "# array of all the characters\n",
    "characters = alphabet + symbols\n",
    "\n",
    "index2char = {}\n",
    "number = 0\n",
    "for char in characters: \n",
    "    index2char[number] = char\n",
    "    number += 1\n",
    "\n",
    "print(index2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the number of each character in a dataset\n",
    "def num_chars(dataset, index2char):\n",
    "    chars = {}\n",
    "    for _, label in dataset:\n",
    "        char = index2char[int(torch.argmax(label))]\n",
    "        # update\n",
    "        if char in chars:\n",
    "            chars[char] += 1\n",
    "        # initialize\n",
    "        else:\n",
    "            chars[char] = 1\n",
    "    return chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"'\": 52, ':': 58, ',': 51, '!': 50, '#': 83, '.': 52, '?': 55, ';': 58, '-': 54, 'a': 203, 'b': 188, 'c': 201, 'd': 237, 'e': 273, 'f': 227, 'g': 58, 'h': 75, 'i': 64, 'k': 53, 'l': 67, 'm': 59, 'n': 63, 'o': 69, 'p': 57, 'q': 51, 'r': 65, 's': 59, 't': 79, 'u': 65, 'v': 50, 'w': 58, 'x': 53, 'z': 52, 'j': 50, 'y': 59, ' ': 15, 'C': 67}\n"
     ]
    }
   ],
   "source": [
    "print(num_chars(data, index2char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251\n",
      "105\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "# Create dataloader objects\n",
    "\n",
    "# shuffle all the data\n",
    "random.shuffle(data)\n",
    "\n",
    "# batch sizes for train, test, and validation\n",
    "batch_size_train = 10\n",
    "batch_size_test = 3\n",
    "batch_size_validation = 3\n",
    "\n",
    "# 3130\n",
    "# splitting data to get training, test, and validation sets\n",
    "# change once get more data\n",
    "# 2504 for train\n",
    "train_dataset = data[:2504]\n",
    "# test has 313\n",
    "test_dataset = data[2504:2817]\n",
    "# validation has 313\n",
    "validation_dataset = data[2817:]\n",
    "\n",
    "# create the dataloader objects\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size_test, shuffle=False)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=batch_size_validation, shuffle=True)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(test_loader))\n",
    "print(len(validation_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "{'f': 28, 'i': 6, 'y': 1, 't': 10, 'b': 26, '#': 6, 'e': 35, '.': 6, '-': 6, ':': 6, '?': 5, ',': 5, 'x': 8, 'd': 20, 'a': 21, '!': 2, 'm': 8, 'q': 7, 'h': 8, 'o': 2, 'u': 10, 'l': 9, 'C': 7, 'c': 16, 'n': 6, 'r': 7, 'z': 6, 'j': 5, ';': 4, 'w': 5, 'p': 5, 'v': 3, ' ': 1, 'g': 4, 'k': 2, \"'\": 4, 's': 3}\n"
     ]
    }
   ],
   "source": [
    "print(len(num_chars(test_dataset, index2char)))\n",
    "print(num_chars(test_dataset, index2char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "# to check if a dataset is missing a char\n",
    "test_chars = num_chars(test_dataset, index2char)\n",
    "\n",
    "num = 0\n",
    "for char in characters:\n",
    "    if char in test_chars:\n",
    "        num += 1\n",
    "    else:\n",
    "        break\n",
    "print(num) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (block1): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (block3): Sequential(\n",
      "    (0): Linear(in_features=1568, out_features=100, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=100, out_features=37, bias=True)\n",
      "  )\n",
      ")\n",
      "# parameter:  174685\n"
     ]
    }
   ],
   "source": [
    "# defines the convolutional neural network\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            #3x28x28\n",
    "            nn.Conv2d(in_channels=3, \n",
    "                      out_channels=16, \n",
    "                      kernel_size=5,\n",
    "                      stride=1, \n",
    "                      padding=2),\n",
    "            # batch normalization\n",
    "            # nn.BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True), \n",
    "            #16x28x28\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            #16x14x14\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        #16x14x14\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, \n",
    "                      out_channels=32, \n",
    "                      kernel_size=5,\n",
    "                      stride=1, \n",
    "                      padding=2),\n",
    "            # batch normalization\n",
    "            # nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True), \n",
    "            #32x14x14\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            #32x7x7\n",
    "            nn.LeakyReLU()\n",
    "        ) \n",
    "        # linearly \n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Linear(32*7*7, 100),\n",
    "            # batch normalization\n",
    "            # nn.BatchNorm1d(100),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(100, 37)\n",
    "        )\n",
    "        #1x37\n",
    "    \n",
    "    def forward(self, x): \n",
    "        out = self.block1(x)\n",
    "        out = self.block2(out)\n",
    "        # flatten the dataset\n",
    "        out = out.view(-1, 32*7*7)\n",
    "        out = self.block3(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# convolutional neural network model\n",
    "model = CNN()\n",
    "\n",
    "# print summary of the neural network model to check if everything is fine. \n",
    "print(model)\n",
    "print(\"# parameter: \", sum([param.nelement() for param in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the learning rate\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Using a variable to store the cross entropy method\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Using a variable to store the optimizer \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get which characters were missclassified\n",
    "def get_chars(indices, incorrect_dict, index2char):\n",
    "    for i in indices:\n",
    "        char = index2char[i]\n",
    "        # update\n",
    "        if char in incorrect_dict:\n",
    "            incorrect_dict[char] += 1\n",
    "    return incorrect_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tingyisu/miniconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:49: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training Loss: 3.4815\n",
      "Validation Loss: 3.4333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tingyisu/miniconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:81: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# list of all train_losses \n",
    "train_losses = []\n",
    "\n",
    "# list of all validation losses \n",
    "validation_losses = []\n",
    "\n",
    "# for loop that iterates over all the epochs\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # variables to store/keep track of the loss and number of iterations\n",
    "    train_loss = 0\n",
    "    num_iter_train = 0\n",
    "\n",
    "    # train the model\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate over train_loader\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # need to permute so that the images are of size 3x28x28 \n",
    "        # essential to be able to feed images into the model\n",
    "        images = images.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Zero the gradient buffer\n",
    "        # resets the gradient after each epoch so that the gradients don't add up\n",
    "        optimizer.zero_grad()  \n",
    "\n",
    "        # Forward, get output\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # convert the labels from one hot encoding vectors into integer values \n",
    "        labels = labels.view(-1, 37)\n",
    "        y_true = torch.argmax(labels, 1)\n",
    "\n",
    "        # calculate training loss\n",
    "        loss = criterion(outputs, y_true)\n",
    "        \n",
    "        # Backward (computes all the gradients)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimize\n",
    "        # loops through all parameters and updates weights by using the gradients \n",
    "        # takes steps backwards to optimize (to reach the minimum weight)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update the validation and number of iterations\n",
    "        train_loss += loss.data[0]\n",
    "        num_iter_train += 1\n",
    "\n",
    "    print('Epoch: {}'.format(epoch+1))\n",
    "    print('Training Loss: {:.4f}'.format(train_loss/num_iter_train))\n",
    "    # append training loss over all the epochs\n",
    "    train_losses.append(train_loss/num_iter_train)\n",
    "\n",
    "    # evaluate the model\n",
    "    model.eval()\n",
    "    \n",
    "    # variables to store/keep track of the loss and number of iterations\n",
    "    validation_loss = 0\n",
    "    num_iter_validation = 0\n",
    "    \n",
    "    # Iterate over validation_loader\n",
    "    for i, (images, labels) in enumerate(validation_loader):  \n",
    "        # need to permute so that the images are of size 3x28x28 \n",
    "        # essential to be able to feed images into the model\n",
    "        images = images.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Forward, get output\n",
    "        outputs = model(images)\n",
    "\n",
    "        # convert the labels from one hot encoding vectors to integer values\n",
    "        labels = labels.view(-1, 37)\n",
    "        y_true = torch.argmax(labels, 1)\n",
    "        \n",
    "        # calculate the validation loss\n",
    "        loss = criterion(outputs, y_true)\n",
    "\n",
    "        # update the training loss and number of iterations\n",
    "        validation_loss += loss.data[0]\n",
    "        num_iter_validation += 1\n",
    "\n",
    "    print('Validation Loss: {:.4f}'.format(validation_loss/num_iter_validation))\n",
    "    # append all validation_losses over all the epochs\n",
    "    validation_losses.append(validation_loss/num_iter_validation)\n",
    "\n",
    "    num_iter_test = 0\n",
    "    correct = 0\n",
    "    incorrect_dict = dict([(c, 0) for c in characters]) # initializing an empty dictionary of all the characters\n",
    "    \n",
    "    # Iterate over test_loader\n",
    "    for images, labels in test_loader:  \n",
    "\n",
    "        # need to permute so that the images are of size 3x28x28 \n",
    "        # essential to be able to feed images into the model\n",
    "        images = images.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Forward\n",
    "        outputs = model(images)\n",
    "\n",
    "        # convert the labels from one hot encoding vectors into integer values \n",
    "        labels = labels.view(-1, 37)\n",
    "        y_true = torch.argmax(labels, 1)\n",
    "\n",
    "        # find the index of the prediction\n",
    "        y_pred = torch.argmax(outputs, 1).type('torch.FloatTensor')\n",
    "        \n",
    "        # convert to FloatTensor\n",
    "        y_true = y_true.type('torch.FloatTensor')\n",
    "\n",
    "        # find the mean difference of the comparisons\n",
    "        correct += torch.sum(torch.eq(y_true, y_pred).type('torch.FloatTensor'))\n",
    "\n",
    "        # find missclassified characters\n",
    "        # convert to numpy arrays\n",
    "        np_y_true = np.array(y_true)\n",
    "        np_y_pred = np.array(y_pred)        \n",
    "        # which labels were given for the misclassified characters\n",
    "        incorrect_pred = np_y_pred[np_y_pred != np_y_true]\n",
    "        # what the correct labels should be \n",
    "        incorrect_true = np_y_true[np_y_pred != np_y_true]\n",
    "        \n",
    "        # update incorrect_dict\n",
    "        if len(incorrect_true) > 0:\n",
    "            incorrect_dict = get_chars(incorrect_true, incorrect_dict, index2char)\n",
    "        \n",
    "    print('Accuracy on the test set: {:.4f}%'.format(correct/len(test_dataset) * 100))\n",
    "    print('Number of missclassified characters:', incorrect_dict)\n",
    "    print()\n",
    "\n",
    "# calculate time it took to train the model\n",
    "tf = time.time()\n",
    "print()\n",
    "print(\"time: {} s\" .format(tf-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning curve function\n",
    "def plot_learning_curve(train_losses, validation_losses):\n",
    "    # plot the training and validation losses\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    plt.plot(train_losses, label=\"training\")\n",
    "    plt.plot(validation_losses, label=\"validation\")\n",
    "    plt.legend(loc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the learning curve\n",
    "plt.title(\"Learning Curve (Loss vs Number of Epochs)\")\n",
    "plot_learning_curve(train_losses, validation_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook try_numbers_and_symbols.ipynb to script\n",
      "[NbConvertApp] Writing 11691 bytes to try_numbers_and_symbols.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script try_numbers_and_symbols.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
